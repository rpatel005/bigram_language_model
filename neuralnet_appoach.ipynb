{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab7f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e76191",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e5981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3fce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # chars lookup table \n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6b33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40019080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db311dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70366ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9c5900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29f54dca0d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2639],\n",
       "        [-1.0962],\n",
       "        [ 0.0488],\n",
       "        [ 1.5280],\n",
       "        [ 0.9428],\n",
       "        [ 1.4663],\n",
       "        [ 0.0930],\n",
       "        [ 0.6161],\n",
       "        [-0.1599],\n",
       "        [-0.3138],\n",
       "        [-1.9789],\n",
       "        [ 0.6742],\n",
       "        [ 0.9777],\n",
       "        [-1.2340],\n",
       "        [ 1.6835],\n",
       "        [ 0.7286],\n",
       "        [ 0.4532],\n",
       "        [ 0.2955],\n",
       "        [-0.8952],\n",
       "        [-0.3558],\n",
       "        [-0.6962],\n",
       "        [ 1.4437],\n",
       "        [ 0.3605],\n",
       "        [ 0.0684],\n",
       "        [-0.8639],\n",
       "        [-0.9496],\n",
       "        [ 0.9320]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 1) # for single neuron\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9cf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2639],\n",
       "        [ 1.4663],\n",
       "        [-1.2340],\n",
       "        [-1.2340],\n",
       "        [-1.0962]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W # (5, 27) @ (27, 1) all the 5 inputs to a single neuron give 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5009, -1.0749,  0.3621, -0.2274, -0.5257,  0.5697,  1.6048, -0.4791,\n",
       "          0.7015, -1.4450, -0.2986, -0.5871, -2.4313, -0.7902,  1.8175,  0.4611,\n",
       "          0.1552, -0.2877, -0.9993,  0.8712, -0.5104, -2.9654, -0.2875,  0.1584,\n",
       "          0.5875, -1.5110, -0.9658],\n",
       "        [-0.8710,  0.2712,  0.9443,  0.3071, -1.6876,  0.3403, -1.8503, -0.0229,\n",
       "         -2.5632, -0.4480, -1.0621, -1.1363,  1.1259, -1.7196,  0.0290,  0.8072,\n",
       "          0.2782,  0.2518, -0.2583, -0.3411, -0.5437,  0.2915, -0.2630, -1.7232,\n",
       "          1.7657, -0.4456, -1.0595],\n",
       "        [ 0.8710, -0.4420, -1.2965, -1.4392,  0.6664,  1.0814,  1.0295,  1.2525,\n",
       "         -0.1408, -1.5465,  1.3976,  0.5024,  0.2607,  0.3799, -0.5443, -0.5691,\n",
       "         -1.1446, -1.1246, -0.5990,  0.3305, -0.6517,  1.1820,  0.4749,  0.5346,\n",
       "          0.8300, -0.1793,  0.2459],\n",
       "        [ 0.8710, -0.4420, -1.2965, -1.4392,  0.6664,  1.0814,  1.0295,  1.2525,\n",
       "         -0.1408, -1.5465,  1.3976,  0.5024,  0.2607,  0.3799, -0.5443, -0.5691,\n",
       "         -1.1446, -1.1246, -0.5990,  0.3305, -0.6517,  1.1820,  0.4749,  0.5346,\n",
       "          0.8300, -0.1793,  0.2459],\n",
       "        [-0.4029,  0.5005,  1.1872, -0.4542, -1.4096,  0.0136, -0.1548,  1.6681,\n",
       "          0.7436, -1.4463, -0.4517, -0.6783,  1.8981,  1.6795,  1.3414, -1.1380,\n",
       "          0.5946,  0.3329, -0.4587,  1.0401, -1.1461, -1.4395, -0.1804, -0.0710,\n",
       "          1.5459, -0.2509, -0.0309]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(27, 27) # 27 neurons\n",
    "xenc @ W # (5, 27) @ (27, 27) --> (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227c031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2607)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3, 12] # firing of 12th neuron looking at 3rd input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b824c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4859, 0.3413, 1.4364, 0.7966, 0.5912, 1.7677, 4.9770, 0.6193, 2.0168,\n",
      "         0.2357, 0.7418, 0.5559, 0.0879, 0.4538, 6.1567, 1.5859, 1.1679, 0.7500,\n",
      "         0.3681, 2.3898, 0.6002, 0.0515, 0.7501, 1.1717, 1.7995, 0.2207, 0.3807],\n",
      "        [0.4185, 1.3115, 2.5710, 1.3595, 0.1850, 1.4054, 0.1572, 0.9774, 0.0771,\n",
      "         0.6389, 0.3457, 0.3210, 3.0831, 0.1791, 1.0295, 2.2416, 1.3208, 1.2863,\n",
      "         0.7724, 0.7110, 0.5806, 1.3384, 0.7688, 0.1785, 5.8454, 0.6405, 0.3466],\n",
      "        [2.3894, 0.6427, 0.2735, 0.2371, 1.9472, 2.9488, 2.7998, 3.4992, 0.8687,\n",
      "         0.2130, 4.0453, 1.6527, 1.2978, 1.4621, 0.5802, 0.5660, 0.3183, 0.3248,\n",
      "         0.5494, 1.3916, 0.5211, 3.2608, 1.6079, 1.7067, 2.2933, 0.8359, 1.2788],\n",
      "        [2.3894, 0.6427, 0.2735, 0.2371, 1.9472, 2.9488, 2.7998, 3.4992, 0.8687,\n",
      "         0.2130, 4.0453, 1.6527, 1.2978, 1.4621, 0.5802, 0.5660, 0.3183, 0.3248,\n",
      "         0.5494, 1.3916, 0.5211, 3.2608, 1.6079, 1.7067, 2.2933, 0.8359, 1.2788],\n",
      "        [0.6684, 1.6495, 3.2780, 0.6349, 0.2443, 1.0137, 0.8565, 5.3020, 2.1034,\n",
      "         0.2354, 0.6366, 0.5075, 6.6732, 5.3627, 3.8245, 0.3205, 1.8122, 1.3951,\n",
      "         0.6321, 2.8295, 0.3179, 0.2371, 0.8349, 0.9314, 4.6923, 0.7781, 0.9696]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1229, 0.0094, 0.0394, 0.0218, 0.0162, 0.0484, 0.1364, 0.0170, 0.0553,\n",
       "         0.0065, 0.0203, 0.0152, 0.0024, 0.0124, 0.1687, 0.0434, 0.0320, 0.0205,\n",
       "         0.0101, 0.0655, 0.0164, 0.0014, 0.0206, 0.0321, 0.0493, 0.0060, 0.0104],\n",
       "        [0.0139, 0.0436, 0.0854, 0.0452, 0.0061, 0.0467, 0.0052, 0.0325, 0.0026,\n",
       "         0.0212, 0.0115, 0.0107, 0.1025, 0.0060, 0.0342, 0.0745, 0.0439, 0.0427,\n",
       "         0.0257, 0.0236, 0.0193, 0.0445, 0.0255, 0.0059, 0.1943, 0.0213, 0.0115],\n",
       "        [0.0605, 0.0163, 0.0069, 0.0060, 0.0493, 0.0746, 0.0709, 0.0886, 0.0220,\n",
       "         0.0054, 0.1024, 0.0418, 0.0328, 0.0370, 0.0147, 0.0143, 0.0081, 0.0082,\n",
       "         0.0139, 0.0352, 0.0132, 0.0825, 0.0407, 0.0432, 0.0580, 0.0212, 0.0324],\n",
       "        [0.0605, 0.0163, 0.0069, 0.0060, 0.0493, 0.0746, 0.0709, 0.0886, 0.0220,\n",
       "         0.0054, 0.1024, 0.0418, 0.0328, 0.0370, 0.0147, 0.0143, 0.0081, 0.0082,\n",
       "         0.0139, 0.0352, 0.0132, 0.0825, 0.0407, 0.0432, 0.0580, 0.0212, 0.0324],\n",
       "        [0.0137, 0.0338, 0.0673, 0.0130, 0.0050, 0.0208, 0.0176, 0.1088, 0.0432,\n",
       "         0.0048, 0.0131, 0.0104, 0.1369, 0.1100, 0.0785, 0.0066, 0.0372, 0.0286,\n",
       "         0.0130, 0.0581, 0.0065, 0.0049, 0.0171, 0.0191, 0.0963, 0.0160, 0.0199]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc @ W) # log-counts\n",
    "counts = logits.exp() # somehting that is equivalent to counts in original plot of next char prediction\n",
    "print(counts)\n",
    "prob = counts / counts.sum(1, keepdims=True) # normalize the counts/rows\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaef77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-------------- SUMMARY---------- ##\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53e1db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f241c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fde484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n",
    "## The big deal here is we have loss function and the loss is made up of only differentiable operation.\n",
    "## Now we can minimize the loss by tuning W (wts) by computing gradients of the loss wrt W-matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e57c833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------- OPTIMIZATION --------------#\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "638c5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # (!important) check pytorch for 'requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acc591fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a801962b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(5), ys] # probs that nn assignes to the correct chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c9a39c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7492, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0391ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Backward pass-----------#\n",
    "W.grad = None # set to the zero gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92f62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cf18553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# Final summary #\n",
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.586362600326538\n",
      "2.586362600326538\n",
      "2.586362600326538\n",
      "2.586362600326538\n",
      "2.586362600326538\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(5):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() ## regularization part = + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "95bb9058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momakurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
